
# Update and Install Required Packages
# -------------------------------------
# 1. Update the package list to get the latest version of available packages.
sudo apt-get update

# 2. Install Python's package manager, pip.
sudo apt install python3-pip

# 3. Install Python's virtual environment tool.
sudo apt install python3-venv

# 4. Install OpenJDK 11, which is required for Hadoop and Spark.
sudo apt install openjdk-11-jdk -y


# Hadoop Installation
# ---------------------
# 1. Download Hadoop version 3.3.6.
wget https://downloads.apache.org/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz

# 2. Extract the downloaded tar file.
tar -xvzf hadoop-3.3.6.tar.gz

# 3. Move the extracted Hadoop folder to '/usr/local/hadoop'.
sudo mv hadoop-3.3.6 /usr/local/hadoop

# 4. Open the .bashrc file to set Hadoop environment variables.
nano ~/.bashrc

# Add the following lines at the end of the .bashrc file:
# Hadoop Environment Variables
export HADOOP_HOME=/usr/local/hadoop
export PATH=$PATH:$HADOOP_HOME/bin
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64

# 5. Apply the changes in the .bashrc file.
source ~/.bashrc

# 6. Verify the Hadoop installation.
hadoop version


# Python Virtual Environment Setup
# ----------------------------------
# 1. Create a Python virtual environment named 'venv'.
python3 -m venv venv

# 2. Activate the virtual environment.
source venv/bin/activate

# 3. Upgrade pip to the latest version.
pip install --upgrade pip

# 4. Install Jupyter Notebook and PySpark dependencies.
pip install jupyter
pip install pyspark findspark
pip install ipykernel


# Spark Installation
# --------------------
# 1. Download Spark version 3.5.3 pre-built for Hadoop 3.
wget https://downloads.apache.org/spark/spark-3.5.3/spark-3.5.3-bin-hadoop3.tgz

# 2. Extract the downloaded tar file.
tar -xvzf spark-3.5.3-bin-hadoop3.tgz

# 3. Move the extracted Spark folder to '/usr/local/spark'.
sudo mv spark-3.5.3-bin-hadoop3 /usr/local/spark

# 4. Open the .bashrc file to set Spark environment variables.
nano ~/.bashrc

# Add the following lines at the end of the .bashrc file:
# Spark Environment Variables
export SPARK_HOME=/usr/local/spark
export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin

# 5. Apply the changes in the .bashrc file.
source ~/.bashrc

# 6. Launch the Spark shell to verify the installation.
spark-shell


# Jupyter Notebook Configuration for PySpark
# --------------------------------------------
# 1. Activate the Python virtual environment.
source venv/bin/activate

# 2. Install the PySpark kernel for Jupyter Notebook.
python3 -m ipykernel install --user --name=pyspark --display-name "PySpark"

# 3. Generate Jupyter Notebook configuration file.
jupyter notebook --generate-config

# 4. Navigate to the Jupyter configuration directory.
cd .jupyter/

# 5. Open the configuration file to allow remote access.
nano jupyter_notebook_config.py

# Add the following lines to configure Jupyter Notebook:
c.NotebookApp.ip = '*'
c.NotebookApp.open_browser = False

# Save and exit (Ctrl + X, then Y, then Enter).

# 6. Start Jupyter Notebook.
jupyter notebook

# Jupyter Notebook should now be accessible at the following URL:
# http://<YOUR_VM_IP>:8888/tree?token=<GENERATED_TOKEN>

